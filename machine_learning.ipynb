{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENSEMBLE LEARNING.**\n",
    "\n",
    "Technique createsmultiple models and then combines them to produce better results than any of the single models\n",
    "\n",
    "**Random Forest**\n",
    "        - Can be used for classification and regression\n",
    "        - Easily handles outlier\n",
    "        - Accepts various types of inputs(Continuos, ordinal)\n",
    "        - Less likely to overfit\n",
    "        - Outputs feature importances."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CLEANING UP OUR DATAAND INTRODUCING FEATURES TO THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, i,...</td>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, dont, think, goe, usf, live, around, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, brother, like, speak, they, treat, like...</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                        cleaned_text  body_len  punc%  \n",
       "0  [ive, search, right, word, thank, breather, i,...       160    2.5  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...       128    4.7  \n",
       "2  [nah, i, dont, think, goe, usf, live, around, ...        49    4.1  \n",
       "3  [even, brother, like, speak, they, treat, like...        62    3.2  \n",
       "4         [i, have, a, date, on, sunday, with, will]        28    7.1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize,word_tokenize\n",
    "import os\n",
    "import re\n",
    "\n",
    "dataset = pd.read_csv(os.getcwd()+'\\\\Ex_Files_NLP_Python_ML_EssT\\\\Exercise Files\\\\Ch01\\\\01_03\\\\Start\\\\SMSSpamCollection.tsv', header=None, sep='\\t')\n",
    "dataset.columns = ['label', 'body_text']\n",
    "dataset.sample(5)\n",
    "\n",
    "#create new features containing the percentage of punctuation in text and the body length of text \n",
    "\n",
    "def count_punctuation_and_body_len(text):\n",
    "    punct_count = [char for char in text if char in string.punctuation]\n",
    "    body_len = len(text) - text.count(\" \")\n",
    "    punct_perc = round(len(punct_count)/body_len, 3)*100\n",
    "    return body_len, punct_perc\n",
    "\n",
    "sp_words = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "def clean_data(text):\n",
    "    tokens = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokens_punct = re.split('\\s+', tokens)\n",
    "    tokens_stop = [token.lower()  for token in tokens_punct if token not in sp_words]\n",
    "    clean_text = [ps.stem(word) for word in tokens_stop]\n",
    "    return clean_text\n",
    "\n",
    "dataset['cleaned_text'] = dataset['body_text'].apply(lambda x: clean_data(x))\n",
    "dataset['body_len'] = dataset['body_text'].apply(lambda x: count_punctuation_and_body_len(x)[0])\n",
    "dataset['punc%'] = dataset['body_text'].apply(lambda x: count_punctuation_and_body_len(x)[1])    \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "APPLY VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punc%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8265</th>\n",
       "      <th>8266</th>\n",
       "      <th>8267</th>\n",
       "      <th>8268</th>\n",
       "      <th>8269</th>\n",
       "      <th>8270</th>\n",
       "      <th>8271</th>\n",
       "      <th>8272</th>\n",
       "      <th>8273</th>\n",
       "      <th>8274</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8277 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punc%    0    1    2    3    4    5    6    7  ...   8265  8266  \\\n",
       "0       160    2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "1       128    4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "2        49    4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "3        62    3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "4        28    7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "\n",
       "   8267  8268  8269  8270  8271  8272  8273  8274  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8277 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def return_cleaned_text_string(text):\n",
    "    string_version = ''.join(text)\n",
    "    return string_version\n",
    "\n",
    "dataset['body_text_new'] = dataset['cleaned_text'].apply(lambda x: return_cleaned_text_string(x))\n",
    "\n",
    "#tfidf\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_data)\n",
    "x_tfidf = tfidf_vect.fit_transform(dataset['body_text'])\n",
    "x_tfidf_features = pd.concat([dataset[['body_len','punc%']], pd.DataFrame(x_tfidf.toarray())], axis=1)\n",
    "#count\n",
    "count_vect = CountVectorizer(analyzer=clean_data)\n",
    "x_count = count_vect.fit_transform(dataset['body_text'])\n",
    "x_count_features = pd.concat([dataset[['body_len','punc%']], pd.DataFrame(x_tfidf.toarray())], axis=1)\n",
    "\n",
    "x_count_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_estimator_type', '_get_param_names', '_make_estimator', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'decision_path', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params']\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print (dir(RandomForestClassifier))\n",
    "print (RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RUNNING RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibMemoryError",
     "evalue": "JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 20, 22, 10, 649709, tzinfo=tzutc()), 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'4C6F51297AC04F2C84F0078A9AB4684E']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 20, 22, 10, 649709, tzinfo=tzutc()), 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'4C6F51297AC04F2C84F0078A9AB4684E'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 20, 22, 10, 649709, tzinfo=tzutc()), 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-65-8e04522c1e8d>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1f197761400, executio..._before_exec=None error_in_exec=None result=None>)\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n   2807                 code = compiler(mod, cell_name, \"single\")\n-> 2808                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001F1C5E6E4B0, file \"<ipython-input-65-8e04522c1e8d>\", line 6>\n        result = <ExecutionResult object at 1f197761400, executio..._before_exec=None error_in_exec=None result=None>\n   2809                     return True\n   2810 \n   2811             # Flush softspace\n   2812             if softspace(sys.stdout, 0):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001F1C5E6E4B0, file \"<ipython-input-65-8e04522c1e8d>\", line 6>, result=<ExecutionResult object at 1f197761400, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001F1C5E6E4B0, file \"<ipython-input-65-8e04522c1e8d>\", line 6>\n        self.user_global_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], '_10': ['Hello', 'Micheal', '!', 'x'], ...}\n        self.user_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], '_10': ['Hello', 'Micheal', '!', 'x'], ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\user\\Documents\\NLTK\\<ipython-input-65-8e04522c1e8d> in <module>()\n      1 #randomforest in action\n      2 from sklearn.model_selection import KFold, cross_val_score\n      3 \n      4 rf = RandomForestClassifier(n_jobs=-1)\n      5 k_fold = KFold(n_splits=5)\n----> 6 cross_val_score(rf, x_features, dataset['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in cross_val_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, scoring='accuracy', cv=KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in cross_validate(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, scoring={'score': make_scorer(accuracy_score)}, cv=KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=5, random_state=None, shuffle=False)>\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Tue Nov 20 20:22:47 2018\nPID: 23128                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': make_scorer(accuracy_score)}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2228, 2229, 2230, ..., 3339, 3340, 3341]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        y_train = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object, sample_weight=None)\n    242         -------\n    243         self : object\n    244             Returns self.\n    245         \"\"\"\n    246         # Validate or convert input data\n--> 247         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    248         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n    249         if sample_weight is not None:\n    250             sample_weight = check_array(sample_weight, ensure_2d=False)\n    251         if issparse(X):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], accept_sparse='csc', dtype=<class 'numpy.float32'>, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    428 \n    429     if sp.issparse(array):\n    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n    431                                       force_all_finite)\n    432     else:\n--> 433         array = np.array(array, dtype=dtype, order=order, copy=copy)\n        array =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        dtype = <class 'numpy.float32'>\n        order = None\n        copy = False\n    434 \n    435         if ensure_2d:\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in __array__(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], dtype=dtype('float32'))\n    980 \n    981     # ----------------------------------------------------------------------\n    982     # Array Interface\n    983 \n    984     def __array__(self, dtype=None):\n--> 985         return _values_from_object(self)\n        self =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    986 \n    987     def __array_wrap__(self, result, context=None):\n    988         d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n    989         return self._constructor(result, **d).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.cp36-win_amd64.pyd in pandas._libs.lib.values_from_object()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in get_values(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns])\n   3281         # compat\n   3282         return self.as_matrix()\n   3283 \n   3284     def get_values(self):\n   3285         \"\"\"same as values (but handles sparseness conversions)\"\"\"\n-> 3286         return self.as_matrix()\n        self.as_matrix = <bound method NDFrame.as_matrix of       body_le...   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]>\n   3287 \n   3288     def get_dtype_counts(self):\n   3289         \"\"\"Return the counts of dtypes in this object.\"\"\"\n   3290         from pandas import Series\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in as_matrix(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], columns=None)\n   3248         --------\n   3249         pandas.DataFrame.values\n   3250         \"\"\"\n   3251         self._consolidate_inplace()\n   3252         if self._AXIS_REVERSED:\n-> 3253             return self._data.as_matrix(columns).T\n        self._data.as_matrix = <bound method BlockManager.as_matrix of BlockMan...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n        columns.T = undefined\n   3254         return self._data.as_matrix(columns)\n   3255 \n   3256     @property\n   3257     def values(self):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in as_matrix(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64, items=None)\n   3445             mgr = self\n   3446 \n   3447         if self._is_single_block or not self.is_mixed_type:\n   3448             return mgr.blocks[0].get_values()\n   3449         else:\n-> 3450             return mgr._interleave()\n        mgr._interleave = <bound method BlockManager._interleave of BlockM...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n   3451 \n   3452     def _interleave(self):\n   3453         \"\"\"\n   3454         Return ndarray from blocks with specified item order\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _interleave(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64)\n   3454         Return ndarray from blocks with specified item order\n   3455         Items must be contained in the blocks\n   3456         \"\"\"\n   3457         dtype = _interleaved_dtype(self.blocks)\n   3458 \n-> 3459         result = np.empty(self.shape, dtype=dtype)\n        result = undefined\n        self.shape = (8277, 4454)\n        dtype = dtype('float64')\n   3460 \n   3461         if result.shape[0] == 0:\n   3462             # Workaround for numpy 1.7 bug:\n   3463             #\n\nMemoryError: \n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 247, in fit\n    X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 433, in check_array\n    array = np.array(array, dtype=dtype, order=order, copy=copy)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 985, in __array__\n    return _values_from_object(self)\n  File \"pandas\\_libs\\lib.pyx\", line 93, in pandas._libs.lib.values_from_object\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3286, in get_values\n    return self.as_matrix()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3253, in as_matrix\n    return self._data.as_matrix(columns).T\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 3450, in as_matrix\n    return mgr._interleave()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 3459, in _interleave\n    result = np.empty(self.shape, dtype=dtype)\nMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nMemoryError                                        Tue Nov 20 20:22:47 2018\nPID: 23128                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': make_scorer(accuracy_score)}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2228, 2229, 2230, ..., 3339, 3340, 3341]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        y_train = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object, sample_weight=None)\n    242         -------\n    243         self : object\n    244             Returns self.\n    245         \"\"\"\n    246         # Validate or convert input data\n--> 247         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    248         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n    249         if sample_weight is not None:\n    250             sample_weight = check_array(sample_weight, ensure_2d=False)\n    251         if issparse(X):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], accept_sparse='csc', dtype=<class 'numpy.float32'>, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    428 \n    429     if sp.issparse(array):\n    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n    431                                       force_all_finite)\n    432     else:\n--> 433         array = np.array(array, dtype=dtype, order=order, copy=copy)\n        array =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        dtype = <class 'numpy.float32'>\n        order = None\n        copy = False\n    434 \n    435         if ensure_2d:\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in __array__(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], dtype=dtype('float32'))\n    980 \n    981     # ----------------------------------------------------------------------\n    982     # Array Interface\n    983 \n    984     def __array__(self, dtype=None):\n--> 985         return _values_from_object(self)\n        self =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    986 \n    987     def __array_wrap__(self, result, context=None):\n    988         d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n    989         return self._constructor(result, **d).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.cp36-win_amd64.pyd in pandas._libs.lib.values_from_object()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in get_values(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns])\n   3281         # compat\n   3282         return self.as_matrix()\n   3283 \n   3284     def get_values(self):\n   3285         \"\"\"same as values (but handles sparseness conversions)\"\"\"\n-> 3286         return self.as_matrix()\n        self.as_matrix = <bound method NDFrame.as_matrix of       body_le...   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]>\n   3287 \n   3288     def get_dtype_counts(self):\n   3289         \"\"\"Return the counts of dtypes in this object.\"\"\"\n   3290         from pandas import Series\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in as_matrix(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], columns=None)\n   3248         --------\n   3249         pandas.DataFrame.values\n   3250         \"\"\"\n   3251         self._consolidate_inplace()\n   3252         if self._AXIS_REVERSED:\n-> 3253             return self._data.as_matrix(columns).T\n        self._data.as_matrix = <bound method BlockManager.as_matrix of BlockMan...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n        columns.T = undefined\n   3254         return self._data.as_matrix(columns)\n   3255 \n   3256     @property\n   3257     def values(self):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in as_matrix(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64, items=None)\n   3445             mgr = self\n   3446 \n   3447         if self._is_single_block or not self.is_mixed_type:\n   3448             return mgr.blocks[0].get_values()\n   3449         else:\n-> 3450             return mgr._interleave()\n        mgr._interleave = <bound method BlockManager._interleave of BlockM...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n   3451 \n   3452     def _interleave(self):\n   3453         \"\"\"\n   3454         Return ndarray from blocks with specified item order\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _interleave(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64)\n   3454         Return ndarray from blocks with specified item order\n   3455         Items must be contained in the blocks\n   3456         \"\"\"\n   3457         dtype = _interleaved_dtype(self.blocks)\n   3458 \n-> 3459         result = np.empty(self.shape, dtype=dtype)\n        result = undefined\n        self.shape = (8277, 4454)\n        dtype = dtype('float64')\n   3460 \n   3461         if result.shape[0] == 0:\n   3462             # Workaround for numpy 1.7 bug:\n   3463             #\n\nMemoryError: \n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nMemoryError                                        Tue Nov 20 20:22:47 2018\nPID: 23128                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': make_scorer(accuracy_score)}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2228, 2229, 2230, ..., 3339, 3340, 3341]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        y_train = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object, sample_weight=None)\n    242         -------\n    243         self : object\n    244             Returns self.\n    245         \"\"\"\n    246         # Validate or convert input data\n--> 247         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    248         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n    249         if sample_weight is not None:\n    250             sample_weight = check_array(sample_weight, ensure_2d=False)\n    251         if issparse(X):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], accept_sparse='csc', dtype=<class 'numpy.float32'>, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    428 \n    429     if sp.issparse(array):\n    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n    431                                       force_all_finite)\n    432     else:\n--> 433         array = np.array(array, dtype=dtype, order=order, copy=copy)\n        array =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        dtype = <class 'numpy.float32'>\n        order = None\n        copy = False\n    434 \n    435         if ensure_2d:\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in __array__(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], dtype=dtype('float32'))\n    980 \n    981     # ----------------------------------------------------------------------\n    982     # Array Interface\n    983 \n    984     def __array__(self, dtype=None):\n--> 985         return _values_from_object(self)\n        self =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    986 \n    987     def __array_wrap__(self, result, context=None):\n    988         d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n    989         return self._constructor(result, **d).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.cp36-win_amd64.pyd in pandas._libs.lib.values_from_object()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in get_values(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns])\n   3281         # compat\n   3282         return self.as_matrix()\n   3283 \n   3284     def get_values(self):\n   3285         \"\"\"same as values (but handles sparseness conversions)\"\"\"\n-> 3286         return self.as_matrix()\n        self.as_matrix = <bound method NDFrame.as_matrix of       body_le...   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]>\n   3287 \n   3288     def get_dtype_counts(self):\n   3289         \"\"\"Return the counts of dtypes in this object.\"\"\"\n   3290         from pandas import Series\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in as_matrix(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], columns=None)\n   3248         --------\n   3249         pandas.DataFrame.values\n   3250         \"\"\"\n   3251         self._consolidate_inplace()\n   3252         if self._AXIS_REVERSED:\n-> 3253             return self._data.as_matrix(columns).T\n        self._data.as_matrix = <bound method BlockManager.as_matrix of BlockMan...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n        columns.T = undefined\n   3254         return self._data.as_matrix(columns)\n   3255 \n   3256     @property\n   3257     def values(self):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in as_matrix(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64, items=None)\n   3445             mgr = self\n   3446 \n   3447         if self._is_single_block or not self.is_mixed_type:\n   3448             return mgr.blocks[0].get_values()\n   3449         else:\n-> 3450             return mgr._interleave()\n        mgr._interleave = <bound method BlockManager._interleave of BlockM...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n   3451 \n   3452     def _interleave(self):\n   3453         \"\"\"\n   3454         Return ndarray from blocks with specified item order\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _interleave(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64)\n   3454         Return ndarray from blocks with specified item order\n   3455         Items must be contained in the blocks\n   3456         \"\"\"\n   3457         dtype = _interleaved_dtype(self.blocks)\n   3458 \n-> 3459         result = np.empty(self.shape, dtype=dtype)\n        result = undefined\n        self.shape = (8277, 4454)\n        dtype = dtype('float64')\n   3460 \n   3461         if result.shape[0] == 0:\n   3462             # Workaround for numpy 1.7 bug:\n   3463             #\n\nMemoryError: \n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-8e04522c1e8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m: JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 20, 22, 10, 649709, tzinfo=tzutc()), 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'4C6F51297AC04F2C84F0078A9AB4684E']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 20, 22, 10, 649709, tzinfo=tzutc()), 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'4C6F51297AC04F2C84F0078A9AB4684E'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 20, 22, 10, 649709, tzinfo=tzutc()), 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '62A5DF57D713439E8659569E76364A17', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"#randomforest in action\\nfrom sklearn.model_selec...abel'], cv=k_fold, scoring='accuracy', n_jobs=-1)\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-65-8e04522c1e8d>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1f197761400, executio..._before_exec=None error_in_exec=None result=None>)\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n   2807                 code = compiler(mod, cell_name, \"single\")\n-> 2808                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001F1C5E6E4B0, file \"<ipython-input-65-8e04522c1e8d>\", line 6>\n        result = <ExecutionResult object at 1f197761400, executio..._before_exec=None error_in_exec=None result=None>\n   2809                     return True\n   2810 \n   2811             # Flush softspace\n   2812             if softspace(sys.stdout, 0):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001F1C5E6E4B0, file \"<ipython-input-65-8e04522c1e8d>\", line 6>, result=<ExecutionResult object at 1f197761400, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001F1C5E6E4B0, file \"<ipython-input-65-8e04522c1e8d>\", line 6>\n        self.user_global_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], '_10': ['Hello', 'Micheal', '!', 'x'], ...}\n        self.user_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], '_10': ['Hello', 'Micheal', '!', 'x'], ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\user\\Documents\\NLTK\\<ipython-input-65-8e04522c1e8d> in <module>()\n      1 #randomforest in action\n      2 from sklearn.model_selection import KFold, cross_val_score\n      3 \n      4 rf = RandomForestClassifier(n_jobs=-1)\n      5 k_fold = KFold(n_splits=5)\n----> 6 cross_val_score(rf, x_features, dataset['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in cross_val_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, scoring='accuracy', cv=KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in cross_validate(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, scoring={'score': make_scorer(accuracy_score)}, cv=KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=5, random_state=None, shuffle=False)>\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Tue Nov 20 20:22:47 2018\nPID: 23128                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': make_scorer(accuracy_score)}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2228, 2229, 2230, ..., 3339, 3340, 3341]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': make_scorer(accuracy_score)}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2228, 2229, 2230, ..., 3339, 3340, 3341]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        y_train = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 4454, dtype: object, sample_weight=None)\n    242         -------\n    243         self : object\n    244             Returns self.\n    245         \"\"\"\n    246         # Validate or convert input data\n--> 247         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    248         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n    249         if sample_weight is not None:\n    250             sample_weight = check_array(sample_weight, ensure_2d=False)\n    251         if issparse(X):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], accept_sparse='csc', dtype=<class 'numpy.float32'>, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    428 \n    429     if sp.issparse(array):\n    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n    431                                       force_all_finite)\n    432     else:\n--> 433         array = np.array(array, dtype=dtype, order=order, copy=copy)\n        array =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n        dtype = <class 'numpy.float32'>\n        order = None\n        copy = False\n    434 \n    435         if ensure_2d:\n    436             if array.ndim == 1:\n    437                 raise ValueError(\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in __array__(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], dtype=dtype('float32'))\n    980 \n    981     # ----------------------------------------------------------------------\n    982     # Array Interface\n    983 \n    984     def __array__(self, dtype=None):\n--> 985         return _values_from_object(self)\n        self =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]\n    986 \n    987     def __array_wrap__(self, result, context=None):\n    988         d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n    989         return self._constructor(result, **d).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.cp36-win_amd64.pyd in pandas._libs.lib.values_from_object()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in get_values(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns])\n   3281         # compat\n   3282         return self.as_matrix()\n   3283 \n   3284     def get_values(self):\n   3285         \"\"\"same as values (but handles sparseness conversions)\"\"\"\n-> 3286         return self.as_matrix()\n        self.as_matrix = <bound method NDFrame.as_matrix of       body_le...   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns]>\n   3287 \n   3288     def get_dtype_counts(self):\n   3289         \"\"\"Return the counts of dtypes in this object.\"\"\"\n   3290         from pandas import Series\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in as_matrix(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], columns=None)\n   3248         --------\n   3249         pandas.DataFrame.values\n   3250         \"\"\"\n   3251         self._consolidate_inplace()\n   3252         if self._AXIS_REVERSED:\n-> 3253             return self._data.as_matrix(columns).T\n        self._data.as_matrix = <bound method BlockManager.as_matrix of BlockMan...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n        columns.T = undefined\n   3254         return self._data.as_matrix(columns)\n   3255 \n   3256     @property\n   3257     def values(self):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in as_matrix(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64, items=None)\n   3445             mgr = self\n   3446 \n   3447         if self._is_single_block or not self.is_mixed_type:\n   3448             return mgr.blocks[0].get_values()\n   3449         else:\n-> 3450             return mgr._interleave()\n        mgr._interleave = <bound method BlockManager._interleave of BlockM...IntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64>\n   3451 \n   3452     def _interleave(self):\n   3453         \"\"\"\n   3454         Return ndarray from blocks with specified item order\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _interleave(self=BlockManager\nItems: Index(['body_len',    'punc%...\nIntBlock: slice(0, 1, 1), 1 x 4454, dtype: int64)\n   3454         Return ndarray from blocks with specified item order\n   3455         Items must be contained in the blocks\n   3456         \"\"\"\n   3457         dtype = _interleaved_dtype(self.blocks)\n   3458 \n-> 3459         result = np.empty(self.shape, dtype=dtype)\n        result = undefined\n        self.shape = (8277, 4454)\n        dtype = dtype('float64')\n   3460 \n   3461         if result.shape[0] == 0:\n   3462             # Workaround for numpy 1.7 bug:\n   3463             #\n\nMemoryError: \n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "#randomforest in action\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "k_fold = KFold(n_splits=5)\n",
    "cross_val_score(rf, x_features, dataset['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RUNNING RANDOM FOREST THROUGH HOLDOUT SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.050720577465307733, 1791),\n",
       " (0.04486622923962251, 'body_len'),\n",
       " (0.033798426316406512, 3146),\n",
       " (0.032688908167355014, 6350),\n",
       " (0.032083499222619966, 4842),\n",
       " (0.024182704876344933, 2025),\n",
       " (0.020846811909829639, 7098),\n",
       " (0.01876435512718623, 7259),\n",
       " (0.015553156436916302, 6051),\n",
       " (0.015257888874461292, 7433)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_features, dataset['label'], test_size=0.2)\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.617 / fscore: 0.763 / Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print ('Precision: {} / Recall: {} / fscore: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                                      round(recall, 3),\n",
    "                                                                      round(fscore, 3),\n",
    "                                                                      round((y_pred == y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IMPROVING THE PERFORMANCE OF OUR MODEL BY BUILDING A GRID-SEARCH FUNCTIONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} --- Precision: {} / Recall: {} / Accuracy: {}'.format(n_est, depth,\n",
    "                                                                              round(precision,3), \n",
    "                                                                              round(recall, 3),\n",
    "                                                                            round((y_pred == y_test).sum()/len(y_pred), 3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 --- Precision: 1.0 / Recall: 0.25 / Accuracy: 0.914\n",
      "Est: 10 / Depth: 20 --- Precision: 1.0 / Recall: 0.625 / Accuracy: 0.957\n",
      "Est: 10 / Depth: 30 --- Precision: 1.0 / Recall: 0.75 / Accuracy: 0.971\n",
      "Est: 10 / Depth: None --- Precision: 0.99 / Recall: 0.805 / Accuracy: 0.977\n",
      "Est: 50 / Depth: 10 --- Precision: 1.0 / Recall: 0.25 / Accuracy: 0.914\n",
      "Est: 50 / Depth: 20 --- Precision: 1.0 / Recall: 0.664 / Accuracy: 0.961\n",
      "Est: 50 / Depth: 30 --- Precision: 1.0 / Recall: 0.742 / Accuracy: 0.97\n",
      "Est: 50 / Depth: None --- Precision: 1.0 / Recall: 0.836 / Accuracy: 0.981\n",
      "Est: 100 / Depth: 10 --- Precision: 1.0 / Recall: 0.297 / Accuracy: 0.919\n",
      "Est: 100 / Depth: 20 --- Precision: 1.0 / Recall: 0.617 / Accuracy: 0.956\n",
      "Est: 100 / Depth: 30 --- Precision: 1.0 / Recall: 0.758 / Accuracy: 0.972\n",
      "Est: 100 / Depth: None --- Precision: 1.0 / Recall: 0.867 / Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EVALUATE RANDOM FOREST USING GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibMemoryError",
     "evalue": "JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 3, 455575, tzinfo=tzutc()), 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'4C6F51297AC04F2C84F0078A9AB4684E']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 3, 455575, tzinfo=tzutc()), 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'4C6F51297AC04F2C84F0078A9AB4684E'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 3, 455575, tzinfo=tzutc()), 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Try object>], cell_name='<ipython-input-75-39e4db7536f9>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1f1c5d71c88, executio..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001F1974F96F0, file \"<ipython-input-75-39e4db7536f9>\", line 4>\n        result = <ExecutionResult object at 1f1c5d71c88, executio..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001F1974F96F0, file \"<ipython-input-75-39e4db7536f9>\", line 4>, result=<ExecutionResult object at 1f1c5d71c88, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001F1974F96F0, file \"<ipython-input-75-39e4db7536f9>\", line 4>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\user\\Documents\\NLTK\\<ipython-input-75-39e4db7536f9> in <module>()\n      1 rf = RandomForestClassifier()\n      2 param = {'n_estimators': [10, 150, 300], 'max_depth':[30, 60, 90, None]}\n      3 gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n----> 4 gs_fit = gs.fit(x_tfidf_features, dataset['label'])\n      5 try:\n      6     pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n      7 except Exception as e:\n      8     print(e)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...ain_score='warn',\n       scoring=None, verbose=0), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=5, random_state=None, shuffle=False)>\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Tue Nov 20 21:14:16 2018\nPID: 17228                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 448, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 200, in _safe_split\n    X_subset = safe_indexing(X, indices)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 149, in safe_indexing\n    return X.iloc[indices]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py\", line 1328, in __getitem__\n    return self._getitem_axis(key, axis=0)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py\", line 1738, in _getitem_axis\n    return self._get_list_axis(key, axis=axis)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py\", line 1715, in _get_list_axis\n    return self.obj.take(key, axis=axis, convert=False)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1925, in take\n    self._consolidate_inplace()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3138, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3127, in _protect_consolidate\n    result = f()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3136, in f\n    self._data = self._data.consolidate()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 3573, in consolidate\n    bm._consolidate_inplace()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 3578, in _consolidate_inplace\n    self.blocks = tuple(_consolidate(self.blocks))\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 4525, in _consolidate\n    _can_consolidate=_can_consolidate)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 4548, in _merge_blocks\n    new_values = new_values[argsort]\nMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nMemoryError                                        Tue Nov 20 21:14:16 2018\nPID: 17228                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nMemoryError                                        Tue Nov 20 21:14:16 2018\nPID: 17228                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-39e4db7536f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgs_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tfidf_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m: JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 3, 455575, tzinfo=tzutc()), 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'4C6F51297AC04F2C84F0078A9AB4684E']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 3, 455575, tzinfo=tzutc()), 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'4C6F51297AC04F2C84F0078A9AB4684E'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 3, 455575, tzinfo=tzutc()), 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0BAD458DCC0245E096C952BB7B7387BA', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"rf = RandomForestClassifier()\\nparam = {'n_estima...g=False)[0:5]\\nexcept Exception as e:\\n    print(e)\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Try object>], cell_name='<ipython-input-75-39e4db7536f9>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1f1c5d71c88, executio..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001F1974F96F0, file \"<ipython-input-75-39e4db7536f9>\", line 4>\n        result = <ExecutionResult object at 1f1c5d71c88, executio..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001F1974F96F0, file \"<ipython-input-75-39e4db7536f9>\", line 4>, result=<ExecutionResult object at 1f1c5d71c88, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001F1974F96F0, file \"<ipython-input-75-39e4db7536f9>\", line 4>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\user\\Documents\\NLTK\\<ipython-input-75-39e4db7536f9> in <module>()\n      1 rf = RandomForestClassifier()\n      2 param = {'n_estimators': [10, 150, 300], 'max_depth':[30, 60, 90, None]}\n      3 gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n----> 4 gs_fit = gs.fit(x_tfidf_features, dataset['label'])\n      5 try:\n      6     pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n      7 except Exception as e:\n      8     print(e)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...ain_score='warn',\n       scoring=None, verbose=0), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=5, random_state=None, shuffle=False)>\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Tue Nov 20 21:14:16 2018\nPID: 17228                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300], 'max_depth':[30, 60, 90, None]}\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(x_tfidf_features, dataset['label'])\n",
    "try:\n",
    "    pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibMemoryError",
     "evalue": "JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 46, 243075, tzinfo=tzutc()), 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'4C6F51297AC04F2C84F0078A9AB4684E']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 46, 243075, tzinfo=tzutc()), 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'4C6F51297AC04F2C84F0078A9AB4684E'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 46, 243075, tzinfo=tzutc()), 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-76-ab960fa3c22d>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1f1c496ff98, executio..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001F1C5DBFF60, file \"<ipython-input-76-ab960fa3c22d>\", line 4>\n        result = <ExecutionResult object at 1f1c496ff98, executio..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001F1C5DBFF60, file \"<ipython-input-76-ab960fa3c22d>\", line 4>, result=<ExecutionResult object at 1f1c496ff98, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001F1C5DBFF60, file \"<ipython-input-76-ab960fa3c22d>\", line 4>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\user\\Documents\\NLTK\\<ipython-input-76-ab960fa3c22d> in <module>()\n      1 rf = RandomForestClassifier()\n      2 param = {'n_estimators': [10, 50, 300], 'max_depth':[30, 60, 90, None]}\n      3 gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n----> 4 gs_fit = gs.fit(x_count_features, dataset['label'])\n      5 pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...ain_score='warn',\n       scoring=None, verbose=0), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=5, random_state=None, shuffle=False)>\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Tue Nov 20 21:14:59 2018\nPID: 15952                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 448, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 200, in _safe_split\n    X_subset = safe_indexing(X, indices)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 149, in safe_indexing\n    return X.iloc[indices]\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py\", line 1328, in __getitem__\n    return self._getitem_axis(key, axis=0)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py\", line 1738, in _getitem_axis\n    return self._get_list_axis(key, axis=axis)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py\", line 1715, in _get_list_axis\n    return self.obj.take(key, axis=axis, convert=False)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1925, in take\n    self._consolidate_inplace()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3138, in _consolidate_inplace\n    self._protect_consolidate(f)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3127, in _protect_consolidate\n    result = f()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 3136, in f\n    self._data = self._data.consolidate()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 3573, in consolidate\n    bm._consolidate_inplace()\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 3578, in _consolidate_inplace\n    self.blocks = tuple(_consolidate(self.blocks))\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 4525, in _consolidate\n    _can_consolidate=_can_consolidate)\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py\", line 4548, in _merge_blocks\n    new_values = new_values[argsort]\nMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nMemoryError                                        Tue Nov 20 21:14:59 2018\nPID: 15952                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nMemoryError                                        Tue Nov 20 21:14:59 2018\nPID: 15952                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-ab960fa3c22d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgs_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_count_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m: JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001F18EEBFAE0, fil...Lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\u...a3\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 46, 243075, tzinfo=tzutc()), 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'4C6F51297AC04F2C84F0078A9AB4684E']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 46, 243075, tzinfo=tzutc()), 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'4C6F51297AC04F2C84F0078A9AB4684E'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 20, 21, 14, 46, 243075, tzinfo=tzutc()), 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'session': '4C6F51297AC04F2C84F0078A9AB4684E', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '9FA103EF0A4248DA9D16478D3E80ABC8', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"rf = RandomForestClassifier()\\nparam = {'n_estima...t_values('mean_test_score', ascending=False)[0:5]\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-76-ab960fa3c22d>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1f1c496ff98, executio..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001F1C5DBFF60, file \"<ipython-input-76-ab960fa3c22d>\", line 4>\n        result = <ExecutionResult object at 1f1c496ff98, executio..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001F1C5DBFF60, file \"<ipython-input-76-ab960fa3c22d>\", line 4>, result=<ExecutionResult object at 1f1c496ff98, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001F1C5DBFF60, file \"<ipython-input-76-ab960fa3c22d>\", line 4>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from sklearn.ensemble import RandomForestClassif...restClassifier))\\nprint (RandomForestClassifier())', 'import string \\nimport pandas as pd\\nimport nltk', \"import string \\nimport pandas as pd\\nimport nltk\\n\\n...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...olumns = ['label', 'body_text']\\ndataset.sample(5)\", \"import string \\nimport pandas as pd\\nimport nltk\\nimport os\\n\\nnltk.tokenize('Hello Mike!')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... os\\n\\nx = nltk.tokenize()\\nx.tokenize('Hello Mike')\", \"import string \\nimport pandas as pd\\nimport nltk\\ni... nltk.tokenize.word_tokenize()\\nx('Hello Micheal')\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...e\\nimport os\\n\\nx = 'Hello Micheal'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...\\nimport os\\n\\nx = 'Hello Micheal!'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...import os\\n\\nx = 'Hello Micheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok...mport os\\n\\nx = 'Hello wMicheal!x'\\nword_tokenize(x)\", \"import string \\nimport pandas as pd\\nfrom nltk.tok... os\\n\\nx = 'Hello wMicheal!x'\\nwordpunct_tokenize(x)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...(5)\\n\\nfor i in 'hello, how are you?':\\n    print(i)\", \"import string \\nimport pandas as pd\\nimport nltk\\ni...t']\\ndataset.sample(5)\\n\\nlist('hello, how are you')\", 'import string \\nimport pandas as pd\\nimport nltk\\ni...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...unctuation_and_body_len(x)[1])    \\ndataset.head()', 'import string \\nimport pandas as pd\\nimport nltk\\nf...rdpunct_tokenize(\"I\\'ve come along way, isn\\'t it\")', \"import string \\nimport pandas as pd\\nimport nltk\\nf...\\nf = re.split('\\\\s', x)\\nprint(d, '\\\\n', e, '\\\\n', f)\", ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {4:      label                                      ...Don't Think About \"What u Have Got\" Think Abou..., 8: ['Hello', 'Micheal'], 9: ['Hello', 'Micheal', '!'], 10: ['Hello', 'Micheal', '!', 'x'], 11: ['Hello', 'wMicheal', '!', 'x'], 12: ['Hello', 'wMicheal', '!', 'x'], 14: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u'], 17:   label                                         ...e, on, sunday, with, will, !!]        28    0.0  , 18: ['I', \"'\", 've', 'come', 'along', 'way', ',', 'isn', \"'\", 't', 'it'], 21:   label                                         ...              I HAVE A DATE ON SUNDAY WITH WILL!!, ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test':       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[1114 rows x 8277 columns], 'X_train':       body_len  punc%         0         1    2  ...0   0.0   0.0   0.0  \n\n[4454 rows x 8277 columns], '_':    body_len  punc%    0    1    2    3    4    5... 0.0   0.0   0.0   0.0  \n\n[5 rows x 8277 columns], ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\user\\Documents\\NLTK\\<ipython-input-76-ab960fa3c22d> in <module>()\n      1 rf = RandomForestClassifier()\n      2 param = {'n_estimators': [10, 50, 300], 'max_depth':[30, 60, 90, None]}\n      3 gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n----> 4 gs_fit = gs.fit(x_count_features, dataset['label'])\n      5 pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...ain_score='warn',\n       scoring=None, verbose=0), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=5, random_state=None, shuffle=False)>\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Tue Nov 20 21:14:59 2018\nPID: 15952                 Python 3.6.3: C:\\Users\\user\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False),       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, {'score': <function _passthrough_scorer>}, array([   0,    1,    2, ..., 5565, 5566, 5567]), array([2141, 2142, 2156, ..., 3356, 3378, 3381]), 0, {'max_depth': 30, 'n_estimators': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, scorer={'score': <function _passthrough_scorer>}, train=array([   0,    1,    2, ..., 5565, 5566, 5567]), test=array([2141, 2142, 2156, ..., 3356, 3378, 3381]), verbose=0, parameters={'max_depth': 30, 'n_estimators': 10}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    443     if parameters is not None:\n    444         estimator.set_params(**parameters)\n    445 \n    446     start_time = time.time()\n    447 \n--> 448     X_train, y_train = _safe_split(estimator, X, y, train)\n        X_train = undefined\n        y_train = undefined\n        estimator = RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False)\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        y = 0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object\n        train = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    449     X_test, y_test = _safe_split(estimator, X, y, test, train)\n    450 \n    451     is_multimetric = not callable(scorer)\n    452     n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], y=0        ham\n1       spam\n2        ham\n3        ...     ham\nName: label, Length: 5568, dtype: object, indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), train_indices=None)\n    195         if train_indices is None:\n    196             X_subset = X[np.ix_(indices, indices)]\n    197         else:\n    198             X_subset = X[np.ix_(indices, train_indices)]\n    199     else:\n--> 200         X_subset = safe_indexing(X, indices)\n        X_subset = undefined\n        X =       body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    201 \n    202     if y is not None:\n    203         y_subset = safe_indexing(y, indices)\n    204     else:\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py in safe_indexing(X=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n    144     if hasattr(X, \"iloc\"):\n    145         # Work-around for indexing with read-only indices in pandas\n    146         indices = indices if indices.flags.writeable else indices.copy()\n    147         # Pandas Dataframes and Series\n    148         try:\n--> 149             return X.iloc[indices]\n        X.iloc = <pandas.core.indexing._iLocIndexer object>\n        indices = array([   0,    1,    2, ..., 5565, 5566, 5567])\n    150         except ValueError:\n    151             # Cython typed memoryviews internally used in pandas do not support\n    152             # readonly buffers.\n    153             warnings.warn(\"Copying input dataframe for slicing.\",\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]))\n   1323             except (KeyError, IndexError):\n   1324                 pass\n   1325             return self._getitem_tuple(key)\n   1326         else:\n   1327             key = com._apply_if_callable(key, self.obj)\n-> 1328             return self._getitem_axis(key, axis=0)\n        self._getitem_axis = <bound method _iLocIndexer._getitem_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n   1329 \n   1330     def _is_scalar_access(self, key):\n   1331         raise NotImplementedError()\n   1332 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1733             self._has_valid_type(key, axis)\n   1734             return self._getbool_axis(key, axis=axis)\n   1735 \n   1736         # a list of integers\n   1737         elif is_list_like_indexer(key):\n-> 1738             return self._get_list_axis(key, axis=axis)\n        self._get_list_axis = <bound method _iLocIndexer._get_list_axis of <pandas.core.indexing._iLocIndexer object>>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1739 \n   1740         # a single integer\n   1741         else:\n   1742             key = self._convert_scalar_indexer(key, axis)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py in _get_list_axis(self=<pandas.core.indexing._iLocIndexer object>, key=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0)\n   1710         Returns\n   1711         -------\n   1712         Series object\n   1713         \"\"\"\n   1714         try:\n-> 1715             return self.obj.take(key, axis=axis, convert=False)\n        self.obj.take = <bound method NDFrame.take of       body_len  pu...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        key = array([   0,    1,    2, ..., 5565, 5566, 5567])\n        axis = 0\n   1716         except IndexError:\n   1717             # re-raise with different error message\n   1718             raise IndexError(\"positional indexers are out-of-bounds\")\n   1719 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in take(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], indices=array([   0,    1,    2, ..., 5565, 5566, 5567]), axis=0, convert=False, is_copy=True, **kwargs={})\n   1920         Returns\n   1921         -------\n   1922         taken : type of caller\n   1923         \"\"\"\n   1924         nv.validate_take(tuple(), kwargs)\n-> 1925         self._consolidate_inplace()\n        self._consolidate_inplace = <bound method NDFrame._consolidate_inplace of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n   1926         new_data = self._data.take(indices,\n   1927                                    axis=self._get_block_manager_axis(axis),\n   1928                                    convert=True, verify=True)\n   1929         result = self._constructor(new_data).__finalize__(self)\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _consolidate_inplace(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns])\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n   3136             self._data = self._data.consolidate()\n   3137 \n-> 3138         self._protect_consolidate(f)\n        self._protect_consolidate = <bound method NDFrame._protect_consolidate of   ...   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns]>\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3139 \n   3140     def _consolidate(self, inplace=False):\n   3141         \"\"\"\n   3142         Compute NDFrame with \"consolidated\" internals (data of each dtype\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in _protect_consolidate(self=      body_len  punc%         0    1    2    3  ...0   0.0   0.0   0.0  \n\n[5568 rows x 8277 columns], f=<function NDFrame._consolidate_inplace.<locals>.f>)\n   3122     def _protect_consolidate(self, f):\n   3123         \"\"\"Consolidate _data -- if the blocks have changed, then clear the\n   3124         cache\n   3125         \"\"\"\n   3126         blocks_before = len(self._data.blocks)\n-> 3127         result = f()\n        result = undefined\n        f = <function NDFrame._consolidate_inplace.<locals>.f>\n   3128         if len(self._data.blocks) != blocks_before:\n   3129             self._clear_item_cache()\n   3130         return result\n   3131 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in f()\n   3131 \n   3132     def _consolidate_inplace(self):\n   3133         \"\"\"Consolidate data in place and return None\"\"\"\n   3134 \n   3135         def f():\n-> 3136             self._data = self._data.consolidate()\n   3137 \n   3138         self._protect_consolidate(f)\n   3139 \n   3140     def _consolidate(self, inplace=False):\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in consolidate(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3568         if self.is_consolidated():\n   3569             return self\n   3570 \n   3571         bm = self.__class__(self.blocks, self.axes)\n   3572         bm._is_consolidated = False\n-> 3573         bm._consolidate_inplace()\n        bm._consolidate_inplace = <bound method BlockManager._consolidate_inplace ...: slice(2, 8277, 1), 8275 x 5568, dtype: float64>\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate_inplace(self=BlockManager\nItems: Index(['body_len',    'punc%...k: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3573         bm._consolidate_inplace()\n   3574         return bm\n   3575 \n   3576     def _consolidate_inplace(self):\n   3577         if not self.is_consolidated():\n-> 3578             self.blocks = tuple(_consolidate(self.blocks))\n        self.blocks = (IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64)\n   3579             self._is_consolidated = True\n   3580             self._known_consolidated = True\n   3581             self._rebuild_blknos_and_blklocs()\n   3582 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _consolidate(blocks=(IntBlock: slice(0, 1, 1), 1 x 5568, dtype: int64, FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64))\n   4520     grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n   4521 \n   4522     new_blocks = []\n   4523     for (_can_consolidate, dtype), group_blocks in grouper:\n   4524         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n-> 4525                                       _can_consolidate=_can_consolidate)\n        _can_consolidate = True\n   4526         new_blocks = _extend_blocks(merged_blocks, new_blocks)\n   4527     return new_blocks\n   4528 \n   4529 \n\n...........................................................................\nC:\\Users\\user\\Anaconda3\\Lib\\site-packages\\pandas\\core\\internals.py in _merge_blocks(blocks=[FloatBlock: slice(1, 2, 1), 1 x 5568, dtype: float64, FloatBlock: slice(2, 8277, 1), 8275 x 5568, dtype: float64], dtype='float64', _can_consolidate=True)\n   4543         # combination of those slices is a slice, too.\n   4544         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n   4545         new_values = _vstack([b.values for b in blocks], dtype)\n   4546 \n   4547         argsort = np.argsort(new_mgr_locs)\n-> 4548         new_values = new_values[argsort]\n        new_values = array([[  2.5,   4.7,   4.1, ...,  14.6,   1. , ...[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ]])\n        argsort = array([   0,    1,    2, ..., 8273, 8274, 8275], dtype=int64)\n   4549         new_mgr_locs = new_mgr_locs[argsort]\n   4550 \n   4551         return make_block(new_values, fastpath=True, placement=new_mgr_locs)\n   4552 \n\nMemoryError: \n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 50, 300], 'max_depth':[30, 60, 90, None]}\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(x_count_features, dataset['label'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
